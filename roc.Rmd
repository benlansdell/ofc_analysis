---
title: "ROC analysis"
output: html_notebook
---

## ROC analysis for cells

Goals of notebook:
* Try out ROC analysis on single recording
* Study questions of normalization for method

```{bash}
ls /media/core/core_operations/ImageAnalysisScratch/Schwarz/Cameron/ImagingData/
```

```{r}
fn = 'final_060603.Rds'
```

Load data
```{r}
load(file = sprintf("./intermediate/0_datatablesfor_%s", fn))
```

Imports
```{r}
library(hash)
library(ROCR)

library(foreach)
library(parallel)
library(doParallel)
```

The loaded data table
```{r}
ds
```

### Preprocessing of signal

How much variability is there between trials for a given cell?
```{r}
a = ds[,.(mn = mean(dff), sd = sd(dff)), by = .(CellID, `Trial Number`)]
a[CellID == 1]
```

How should we normalize? In particular, should we normalize by trial?

Some thoughts:

If the question is: is this cell responsive on this particular trial, then we can indeed normalize over each trial and run the ROC analysis over each cell. Although, as argued below, the scaling in this case doesn't matter, so this shouldn't make any difference. Further, on a per trial basis, we have less data to generate useful ROC curves.

Alternatively, if the question is: is this cell responsive over all the trials, then should we normalize by each trial?

This depends on assumptions we make about the data. If we assume the absolute amount of activity is the proxy for response, then normalizing per trial seems to be a bad idea. This is because normalizing each trial separately, then running an ROC analysis over all trials pooled together, will result in magnitudes for trials when the cell is relatively silent being brought up to equal size to trials when the cell is active. If the quiescent trial is indeed not responding to the cue of interest, this is simply adding noise to the signal to be decoded. 

On the other hand, if we assume something like the relative change in activity is the relevant indicator of a cell's response, then normalizing per trial may be a good idea. However, if this were the assumption, then an ROC-based method that just ranks things according to their intensity does not seem like the most appropriate method. Some sort of decoder that took into account the activity relative to a moving average baseline, or something, would be more appropriate. 

Normalize: (z-score)

```{r}
ds[, dff_zs := (dff - mean(dff))/sd(dff), by = `CellID`]
ds
```

### ROC curves measuring responsiveness to events of interest

Compute the ROC curves. The signal is treated as a binary classifier for the experimental state of interest. Here, the states of interest are the first phases of the l/r spout states. After normalizing, we treat the signal as a simple binary classifier. That is, if it is above a given amount then it counts as 1, otherwise 0. We can use this to generate an ROC curve, which indicates how well that cell classifies times as the event of interest or not. The ROC AUC is used as a measure of cell responsiveness. 

Make a dictionary of ROC curves for each cellID
```{r}
roc_curves = hash()
per_cell_ds = split(ds, f = ds$CellID)
cells = unique(ds$CellID)

evevents_of_interest = c("lspout_first", "rspout_first")
```

```{r}
for (evt in events_of_interest) {
  for (dss in per_cell_ds) {
    p = performance(prediction(dss$dff_zs, dss[,..evt]), "tpr", "fpr") 
    q = performance(prediction(dss$dff_zs, dss[,..evt]), "auc") 
    roc_curves[paste(evt, dss[1,CellID])] = list(p, q)
  }
}
```

Plot the ROC curves for all cells
```{r fig1, fig.height = 6, fig.width = 6}
for (evt in events_of_interest) {
  plot(roc_curves[[paste(evt, '1')]][[1]])
  lapply(cells, function(x) plot(roc_curves[[paste(evt,x)]][[1]], add = TRUE));
}
```

Make a new data table to house the results of our analysis.
```{r}
dt_roc = data.table(CellID = cells)
for (evt in events_of_interest){
  aucs = sapply(cells, function(x) roc_curves[[paste(evt, x)]][[2]]@y.values[[1]])
  dt_roc[,eval(paste(evt, 'AUC', sep = "_")) := aucs]
}
dt_roc
```

### Generation of null-distributions for each cell

Now, for each cell, we perform N random shifts of the signal, and recompute the AUC. 

This gives us a null distribution from which we can judge an AUC as significant or not. 

(In the paper [1], N = 1000)

First confirm what the method looks like for a few cells.
```{r}
N = 50

cycle = function (x,i) c(x[-(0:i)], x[0:i])

n_cells = 1
cell <- sample(unique(ds$CellID), n_cells, replace=F)

print("Computing null distributions for AUC for cell:")
print(cell)

dss = ds[CellID == cell]

for (evt in events_of_interest) {
  aucs = c()
  pred_real = performance(prediction(dss$dff_zs, dss[,..evt]), "tpr", "fpr")
  plot(pred_real, colorize = TRUE)
  colname = paste(evt, 'AUC', sep = "_")
  for (j in 1:N) {
    #Perform shuffle of data...
    rand_idx = ceiling(runif(1, 0, length(dss$dff_zs)))
    shuffle = cycle(dss$dff_zs, rand_idx)
    pred = performance(prediction(shuffle, dss[,..evt]), "tpr", "fpr")
    auc = performance(prediction(shuffle, dss[,..evt]), "auc")@y.values[[1]]
    aucs = c(aucs, auc)
    plot(pred, add = TRUE)
  }
  
  this_auc = dt_roc[CellID == cell, ..colname]
  quantile = sum(aucs < this_auc[[1]])/N
  
  plot(pred_real, colorize = TRUE, add = TRUE)
  title(sprintf("Event: %s, Cell %s, AUC: %f, quantile: %f", evt, cell, this_auc, quantile))
}
```

```{r}
hist(aucs, breaks = 10)
title(sprintf("Event: %s, Cell %s", evt, cell))
```

Now repeat for all cells
```{r}
#Create cluster
cl <- makeCluster(20)
registerDoParallel(cl)
```

```{r}
#Either: save each cell data table in file (fread, fwrite), or pass list of cells

#Here we pass a list of cells, and each worker operates on the corresponding data table. 
#This is slow (and memory inefficient) because each worker loads the whole list 'per_cell_dss'

N = 1000
null_rocs = foreach(idx = 1:length(per_cell_ds), .packages = c("data.table", "ROCR")) %dopar% {
  
  dss = per_cell_ds[[idx]]
  cell = dss[1, CellID]
  dt_null = data.table(CellID = cell)
  print(cell)
  for (evt in events_of_interest) {
    aucs = c()
    for (j in 1:N) {
      #Perform shuffle of data...
      rand_idx = ceiling(runif(1, 0, length(dss$dff_zs)))
      shuffle = cycle(dss$dff_zs, rand_idx)
      auc = performance(prediction(shuffle, dss[,..evt]), "auc")@y.values[[1]]
      aucs = c(aucs, auc)
    }
    colname = paste(evt, 'AUC', sep = "_")
    this_auc = dt_roc[CellID == cell, ..colname]
    quantile = sum(aucs < this_auc[[1]])/N
    dt_null[,eval(paste('null_quantile', evt, sep = "_")) := quantile]
  }
  return(dt_null)
}

```

```{r}
null_rocs = rbindlist(null_rocs)
dt_roc[,null_quantile_rspout_first := NULL]
dt_roc[,null_quantile_lspout_first := NULL]
dt_roc = merge(dt_roc, null_rocs, by = 'CellID')
dt_roc
```

```{r}
alpha = 0.05

for (evt in events_of_interest) {
  colname = paste('null_quantile', evt, sep = "_")
  dss = dt_roc[,colname, with = FALSE]
  dt_roc[,(paste(evt, 'pos_resp', sep='_')) := (dss > 1-alpha)]
  dt_roc[,(paste(evt, 'neg_resp', sep='_')) := (dss < alpha)]
}
```

### Summarize the significantly responding cells

How many positively and negatively responding cells are there?
```{r}
for (evt in events_of_interest) {
print(evt)
cn_pos = paste(evt, 'pos_resp', sep = '_')
cn_neg = paste(evt, 'neg_resp', sep = '_')
print(sprintf("Positively responding cells: %d", sum(dt_roc[[cn_pos]])))
print(sprintf("Negatively responding cells: %d", sum(dt_roc[[cn_neg]])))
}

print(sprintf("Out of %d cells", length(cells)))
```

### Plot activity of cells sorted by responsiveness

```{r fig6, fig.height = 10, fig.width = 5}
window = 1000

events_of_interest = c("lspout_first", "rspout_first")
reward = "left"

for (evt in events_of_interest) {
  trials = ds[event == evt, .SD[which.min(frame)], by = `Trial Number`]$`Trial Number`
  
  dss = ds[frame_offset_event > -window & frame_offset_event < window & `Trial Number` %in% trials, .(mn = mean(dff)), by = .(frame_offset_event, CellID)]
  
  #Sort by responsiveness
  dss = merge(dss, dt_roc, by='CellID')
  colname = paste('null_quantile', evt, sep = "_")
  null_quantile = dss[[colname]]
  dss[,null_order := null_quantile]

  p = ggplot(dss, aes(x=frame_offset_event, y=reorder(CellID, null_order), fill = mn)) + 
    geom_tile(show.legend = FALSE) + 
    scale_fill_distiller(palette = "BrBG") + 
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title.y = element_blank()) +
    ggtitle(sprintf("%s (averaged over %d trials; reward is on the %s)", evt, length(trials), reward))

  print(p)  
}
```

### Analysis question: ROC curves, probability and deconvolution

Deconvolution, as used in spike inference methods (e.g. OASIS), will return a signal in the original signal's space. Not interpretable as a probability.

But, an ROC curve need not be computed using thresholds on probabilities. Any score generated by a model/test can be used. For instance in diagnostic medical tests, concentration cutoffs are used. 

### Analysis question: does the normalization method matter?

The ROC curve is invariant to transformations of the signal of the form $f(x) = ax + b, a > 0$ ('orientation preserving affine transformations'). This means any choice of scaling/translation will produce the same ROC curve. And thus, in theory, z-scoring per cell or z-scoring over all cells shouldn't matter.

Why is this the case? A short answer can be seen just by looking at a simple implementation of how to compute the ROC curve:
```
simple_roc <- function(labels, scores){
  Labels <- labels[order(scores, decreasing=TRUE)]
  unique(data.frame(
  FPR=cumsum(!Labels)/sum(!Labels),
  TPR=cumsum(Labels)/sum(Labels))
)}
```
Only the order of the scores matters, not their value. Rescaling doesn't affect the order. 

A more detailed answer is as follows. Let $x$ be the signal, and $z = f(x)$ some transformed signal of the form above. Every point on the ROC curve corresponds to the performance of a classifier obtained by setting the threshold to a given value (or a range of values). Consider the classifier based on $x$: 
$$
g_\theta(x) = \mathbb{I}(x \ge \theta)
$$
Since $f$ is monotonically increasing, each classifier in $x$ has an equivalent classifer in $z$. That is, 
$$
g_\theta(x) = \mathbb{I}(x \ge \theta) = \mathbb{I}(f(x) \ge f(\theta)) = \mathbb{I}(z \ge f(\theta)) = h_{f(\theta)}(z)
$$
and similarly for each classifier in $z$. This means each point on $g$'s ROC curve has an point on $h$'s ROC curve, and vice versa. Thus they are the same curve.

Thus in theory the method is scale invariant. However, when performed numerically, z-scoring for all cells together, for instance, may result in poor resolution of some ROC curves, depending on how the curve is computed. This is because z-scoring all cells together will result in the relatively quiescent cells being at low magnitude at all times. Computing the ROC curves by a threshold common to both a quiet cell and a non-quiet cell will not be numerically well resolved over the quiet cell's smaller range. This is only an issue for the 'naive' implementation of computing an ROC curve (just manually sliding a threshold over the range of the data).

### TODO

* Pool data over multiple recordings
* Consider alternative methods that may also work: 
** Correlation rank
** Bandpass filter before giving to ROC method... removes slow fluctuations in intensity?
** Neural decoding methods? Can decode events as function of neural activity?
** GLM. Could include behavioral variables, as well as events
** Some other statistical analysis that says 'activity is significantly above baseline'
** What else?

### References 

[1] "Cortical Representations of Conspecific Sex Shape Social Behavior" Kingsbury et al 2020