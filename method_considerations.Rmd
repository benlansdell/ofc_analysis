---
title: "Methodological considerations"
output: html_notebook
---

### Analysis question: How should we normalize?

In particular, should we normalize by trial?

Some thoughts:

If the question is: is this cell responsive on this particular trial, then we can indeed normalize over each trial and run the ROC analysis over each cell. Although, as argued below, the scaling in this case doesn't matter, so this shouldn't make any difference. Further, on a per trial basis, we have less data to generate useful ROC curves.

Alternatively, if the question is: is this cell responsive over all the trials, then should we normalize by each trial?

This depends on assumptions we make about the data. If we assume the absolute amount of activity is the proxy for response, then normalizing per trial seems to be a bad idea. This is because normalizing each trial separately, then running an ROC analysis over all trials pooled together, will result in magnitudes for trials when the cell is relatively silent being brought up to equal size to trials when the cell is active. If the quiescent trial is indeed not responding to the cue of interest, this is simply adding noise to the signal to be decoded. 

On the other hand, if we assume something like the relative change in activity is the relevant indicator of a cell's response, then normalizing per trial may be a good idea. However, if this were the assumption, then an ROC-based method that just ranks things according to their intensity does not seem like the most appropriate method. Some sort of decoder that took into account the activity relative to a moving average baseline, or something, would be more appropriate. 

### Analysis question: ROC curves, probability and deconvolution

Deconvolution, as used in spike inference methods (e.g. OASIS), will return a signal in the original signal's space. Not interpretable as a probability.

But, an ROC curve need not be computed using thresholds on probabilities. Any score generated by a model/test can be used. For instance in diagnostic medical tests, concentration cutoffs are used. 

### Analysis question: does the normalization method matter?

The ROC curve is invariant to transformations of the signal of the form $f(x) = ax + b, a > 0$ ('orientation preserving affine transformations'). This means any choice of scaling/translation will produce the same ROC curve. And thus, in theory, z-scoring per cell or z-scoring over all cells shouldn't matter.

Why is this the case? A short answer can be seen just by looking at a simple implementation of how to compute the ROC curve:
```
simple_roc <- function(labels, scores){
  Labels <- labels[order(scores, decreasing=TRUE)]
  unique(data.frame(
  FPR=cumsum(!Labels)/sum(!Labels),
  TPR=cumsum(Labels)/sum(Labels))
)}
```
Only the order of the scores matters, not their value. Rescaling doesn't affect the order. 

A more detailed answer is as follows. Let $x$ be the signal, and $z = f(x)$ some transformed signal of the form above. Every point on the ROC curve corresponds to the performance of a classifier obtained by setting the threshold to a given value (or a range of values). Consider the classifier based on $x$: 
$$
g_\theta(x) = \mathbb{I}(x \ge \theta)
$$
Since $f$ is monotonically increasing, each classifier in $x$ has an equivalent classifer in $z$. That is, 
$$
g_\theta(x) = \mathbb{I}(x \ge \theta) = \mathbb{I}(f(x) \ge f(\theta)) = \mathbb{I}(z \ge f(\theta)) = h_{f(\theta)}(z)
$$
and similarly for each classifier in $z$. This means each point on $g$'s ROC curve has an point on $h$'s ROC curve, and vice versa. Thus they are the same curve.

Thus in theory the method is scale invariant. However, when performed numerically, z-scoring for all cells together, for instance, may result in poor resolution of some ROC curves, depending on how the curve is computed. This is because z-scoring all cells together will result in the relatively quiescent cells being at low magnitude at all times. Computing the ROC curves by a threshold common to both a quiet cell and a non-quiet cell will not be numerically well resolved over the quiet cell's smaller range. This is only an issue for the 'naive' implementation of computing an ROC curve (just manually sliding a threshold over the range of the data).

### Consider alternative methods that may also work: 

* Correlation rank
* Bandpass filter before giving to ROC method... removes slow fluctuations in intensity? Probably don't want this if we're interested in events that last seconds
* Neural decoding methods? Can decode events as a function of neural activity?
* GLM. Could include behavioral variables, as well as events
* Some other statistical analysis that says 'activity is significantly above baseline'